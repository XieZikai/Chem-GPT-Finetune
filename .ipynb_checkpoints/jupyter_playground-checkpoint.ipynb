{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e83b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from data.data_preprocess import generate_classification_dataset, generate_core_train_test_by_equipartition, write_file, generate_classification_dataset_by_equipartition\n",
    "import openai\n",
    "from tuner import Tuner, make_output_dir\n",
    "import json\n",
    "import os\n",
    "from openai.cli import FineTune\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from gpt_attacker import Attacker\n",
    "import math\n",
    "import re\n",
    "import copy\n",
    "from utils import replace_smiles_with_missing\n",
    "import chemprop\n",
    "import tqdm\n",
    "from io import StringIO\n",
    "from utils import SMART_LIST\n",
    "import warnings\n",
    "\n",
    "openai.api_key = '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'\n",
    "COLUMN = 'LUMO'\n",
    "NUM_CLASS = 3\n",
    "SPLIT = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096f1ebe",
   "metadata": {},
   "source": [
    "# 1. Data Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab09b5ba",
   "metadata": {},
   "source": [
    "## Troisi dataset (small molecules) generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb0d009",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = [2, 4, 6, 8]\n",
    "trials = 3\n",
    "datasets = ['HOMO', 'LUMO']\n",
    "\n",
    "for dataset in datasets:\n",
    "    for split in splits:\n",
    "        for trial in range(trials):\n",
    "            df_train, df_test = generate_classification_dataset_by_equipartition(column=dataset, num_class=NUM_CLASS, split=split/10)\n",
    "            write_file(df_train, df_test, '_{}_{}_{}'.format(str(split/10), dataset, str(trial)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145cac5c",
   "metadata": {},
   "source": [
    "## SMPs Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4818ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(x, num_class):\n",
    "    if num_class == 3:\n",
    "        if x < 1.07:\n",
    "            return 'low'\n",
    "        elif x < 12.5:\n",
    "            return 'medium'\n",
    "        else:\n",
    "            return 'high'\n",
    "    elif num_class == 2:\n",
    "        if x < 1.07:\n",
    "            return 'low'\n",
    "        else:\n",
    "            return 'high'\n",
    "        \n",
    "def cut_df(df, n):\n",
    "    df_num = len(df)\n",
    "    every_epoch_num = math.floor((df_num/n))\n",
    "    result = []\n",
    "    for index in range(n):\n",
    "        if index < n-1:\n",
    "            df_tem = df[every_epoch_num * index: every_epoch_num * (index + 1)]\n",
    "        else:\n",
    "            df_tem = df[every_epoch_num * index:]\n",
    "        result.append(df_tem)\n",
    "    return result\n",
    "\n",
    "def generate_smps_dataset(num_class=3, k_fold=5):\n",
    "    df = pd.read_csv('SMPs_572.csv', index_col=0)\n",
    "    df['completion'] = df.apply(lambda x: split(x['HER (Âµmol/h)'], num_class), axis=1)\n",
    "    df = df[['SMILES', 'completion']]\n",
    "    df.columns = ['prompt', 'completion']\n",
    "    df['prompt'] = df.apply(lambda row: row['prompt']+'', axis=1)\n",
    "    result = []\n",
    "    df = df.sample(frac=1.0)\n",
    "    completions = set(df['completion'])\n",
    "    sub_dfs = []\n",
    "    for i in completions:\n",
    "        sub_dfs.append(df[df['completion']==i])\n",
    "        \n",
    "    results = []\n",
    "    \n",
    "    for i in range(k_fold):\n",
    "        train = pd.DataFrame([])\n",
    "        test = pd.DataFrame([])\n",
    "        \n",
    "        for sub_df in sub_dfs:\n",
    "            \n",
    "            sub_df_cut = cut_df(sub_df, k_fold)\n",
    "            test = pd.concat([test, sub_df_cut[i]])\n",
    "            del(sub_df_cut[i])\n",
    "            train = pd.concat([train]+sub_df_cut)\n",
    "        results.append((train, test))\n",
    "    return results\n",
    "\n",
    "a = generate_smps_dataset(2, k_fold=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a146c192",
   "metadata": {},
   "source": [
    "## QMOF dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcef10ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('qmof.csv')\n",
    "df = df[['qmof_id', 'info.mofid.smiles_nodes', 'info.mofid.smiles_linkers', 'info.mofid.topology']]\n",
    "df = df.dropna().reset_index()\n",
    "df['prompt'] = df.apply(lambda x: 'Nodes: '+ ','.join(eval(x['info.mofid.smiles_nodes'])) + ', Linkers: ' + ','.join(eval(x['info.mofid.smiles_linkers'])), axis=1)\n",
    "df['completion'] = df['info.mofid.topology']\n",
    "df = df[['prompt', 'completion', 'qmof_id']]\n",
    "df = df.drop(df[df['completion']=='ERROR,UNKNOWN'].index)\n",
    "\n",
    "# MOF dataset Drop classes\n",
    "\n",
    "d = {}\n",
    "threshold = 10\n",
    "for index, i in enumerate(set(list(df['completion']))):\n",
    "    d[i] = index\n",
    "    di = len(df[df['completion']==i])\n",
    "    if di < threshold:\n",
    "        df = df[df['completion']!=i]\n",
    "print('Class number: ', len(set(list(df['completion']))))\n",
    "\n",
    "label_counts = df['completion'].value_counts()\n",
    "valid_labels = label_counts[label_counts > 5].index\n",
    "df = df[df['completion'].isin(valid_labels)]\n",
    "print(df['completion'].value_counts())\n",
    "\n",
    "mapping = {}\n",
    "for index, i in enumerate(set(list(df['completion']))):\n",
    "    mapping[i] = index\n",
    "    \n",
    "df.to_csv('qmof_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe49435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate train and test set\n",
    "\n",
    "train_index = np.array([])\n",
    "\n",
    "for topo in set(list(df['completion'])):\n",
    "    sub_df = df[df['completion']==topo]\n",
    "    train_num = int(len(sub_df) * SPLIT)\n",
    "    train_index = np.append(train_index, np.random.choice(sub_df.index, size=train_num, replace=False))\n",
    "    \n",
    "df['completion'] = df.apply(lambda x: str(mapping[x['completion']]), axis=1)\n",
    "df = df[['prompt', 'completion']]\n",
    "df_train = df.loc[train_index]\n",
    "df_test = df.append(df_train).drop_duplicates(subset='prompt', keep=False)\n",
    "\n",
    "df_train.to_csv('mof_train.csv')\n",
    "df_test.to_csv('mof_test.csv')\n",
    "with open('mof_class_token_map.json', 'w') as fp:\n",
    "    json.dump(mapping, fp)\n",
    "    \n",
    "write_file(df_train, df_test, 'qmof_topology')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ce3685",
   "metadata": {},
   "source": [
    "# 2. Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2d8f71",
   "metadata": {},
   "source": [
    "## Troisi dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00330ef",
   "metadata": {},
   "source": [
    "## QMOF dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453c21f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e06e7646",
   "metadata": {},
   "source": [
    "# 3. Experiment Results Collection & Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afa27be",
   "metadata": {},
   "source": [
    "## Chemprop result collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bc6dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = r'C:\\Users\\darkn\\PycharmProjects\\ChemGPT\\out\\new_data_gpt\\small_molecule'\n",
    "folders = os.listdir(PATH)\n",
    "\n",
    "results_chemprop = {}\n",
    "\n",
    "# for folder in os.listdir(PATH):\n",
    "for folder in folders:\n",
    "    if not os.path.exists(os.path.join(PATH, folder + '/model_checkpoint/fold_0')):\n",
    "        continue\n",
    "    \n",
    "    arguments = [\n",
    "        '--test_path', os.path.join(PATH, folder + '/valid.csv'),  # \n",
    "        '--checkpoint_dir', os.path.join(PATH, folder + '/model_checkpoint/fold_0'),\n",
    "        '--preds_path',  os.path.join(PATH, folder + '/pred.csv')\n",
    "    ]\n",
    "    args = chemprop.args.PredictArgs().parse_args(arguments)\n",
    "    model_objects = chemprop.train.load_model(args=args)\n",
    "    \n",
    "    \n",
    "    preds = chemprop.train.make_predictions(args=args, model_objects=model_objects)\n",
    "    \n",
    "    df = pd.read_csv(os.path.join(PATH, folder + '/valid.csv'))\n",
    "    y_true = df['target'].to_list()\n",
    "    y_pred = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, pred in enumerate(preds):\n",
    "        total += 1\n",
    "        p = np.argmax(pred)\n",
    "        y_pred.append(p)\n",
    "        if p == y_true[i]:\n",
    "            correct += 1\n",
    "    print (folder, ': ', correct/total)\n",
    "    results_chemprop[folder] = correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd449579",
   "metadata": {},
   "outputs": [],
   "source": [
    "homo_result_chemprop = {}\n",
    "lumo_result_chemprop = {}\n",
    "\n",
    "splits = ['0.2', '0.4', '0.6', '0.8']\n",
    "\n",
    "for split in splits:\n",
    "    keys = [key for key in results_chemprop.keys() if split in key and 'HOMO' in key]\n",
    "    total = 0\n",
    "    for key in keys:\n",
    "        total += results_chemprop[key]\n",
    "    total = total / 3\n",
    "    homo_result_chemprop[split] = total\n",
    "    \n",
    "    keys = [key for key in results_chemprop.keys() if split in key and 'LUMO' in key]\n",
    "    total = 0\n",
    "    for key in keys:\n",
    "        total += results_chemprop[key]\n",
    "    total = total / 3\n",
    "    lumo_result_chemprop[split] = total\n",
    "\n",
    "print(homo_result_chemprop, lumo_result_chemprop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3777bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = r'C:\\Users\\darkn\\PycharmProjects\\ChemGPT\\out\\new_data_gpt\\small_molecule_small_dataset'\n",
    "folders = os.listdir(PATH)\n",
    "\n",
    "results_chemprop = {}\n",
    "\n",
    "# for folder in os.listdir(PATH):\n",
    "for folder in folders:\n",
    "    if not os.path.exists(os.path.join(PATH, folder + '/model_checkpoint/fold_0')):\n",
    "        continue\n",
    "    \n",
    "    arguments = [\n",
    "        '--test_path', os.path.join(PATH, folder + '/valid.csv'),  # \n",
    "        '--checkpoint_dir', os.path.join(PATH, folder + '/model_checkpoint/fold_0'),\n",
    "        '--preds_path',  os.path.join(PATH, folder + '/pred.csv')\n",
    "    ]\n",
    "    args = chemprop.args.PredictArgs().parse_args(arguments)\n",
    "    model_objects = chemprop.train.load_model(args=args)\n",
    "    \n",
    "    \n",
    "    preds = chemprop.train.make_predictions(args=args, model_objects=model_objects)\n",
    "    \n",
    "    df = pd.read_csv(os.path.join(PATH, folder + '/valid.csv'))\n",
    "    y_true = df['target'].to_list()\n",
    "    y_pred = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, pred in enumerate(preds):\n",
    "        total += 1\n",
    "        p = np.argmax(pred)\n",
    "        y_pred.append(p)\n",
    "        if p == y_true[i]:\n",
    "            correct += 1\n",
    "    print (folder, ': ', correct/total)\n",
    "    results_chemprop[folder] = correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14bd36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "homo_result_chemprop = {}\n",
    "lumo_result_chemprop = {}\n",
    "\n",
    "splits = ['0.01', '0.02', '0.1']\n",
    "\n",
    "for split in splits:\n",
    "    keys = [key for key in results_chemprop.keys() if split in key and 'HOMO' in key]\n",
    "    total = 0\n",
    "    for key in keys:\n",
    "        total += results_chemprop[key]\n",
    "    total = total / 3\n",
    "    homo_result_chemprop[split] = total\n",
    "    \n",
    "    keys = [key for key in results_chemprop.keys() if split in key and 'LUMO' in key]\n",
    "    total = 0\n",
    "    for key in keys:\n",
    "        total += results_chemprop[key]\n",
    "    total = total / 3\n",
    "    lumo_result_chemprop[split] = total\n",
    "\n",
    "print(homo_result_chemprop, lumo_result_chemprop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fd9afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = r'C:\\Users\\darkn\\PycharmProjects\\ChemGPT\\out\\new_data_gpt'\n",
    "folders = os.listdir(PATH)\n",
    "\n",
    "results_chemprop = {}\n",
    "\n",
    "# for folder in os.listdir(PATH):\n",
    "for folder in folders:\n",
    "    if not os.path.exists(os.path.join(PATH, folder + '/model_checkpoint/fold_0')):\n",
    "        continue\n",
    "    \n",
    "    arguments = [\n",
    "        '--test_path', os.path.join(PATH, folder + '/valid.csv'),  # \n",
    "        '--checkpoint_dir', os.path.join(PATH, folder + '/model_checkpoint/fold_0'),\n",
    "        '--preds_path',  os.path.join(PATH, folder + '/pred.csv')\n",
    "    ]\n",
    "    args = chemprop.args.PredictArgs().parse_args(arguments)\n",
    "    model_objects = chemprop.train.load_model(args=args)\n",
    "    \n",
    "    \n",
    "    preds = chemprop.train.make_predictions(args=args, model_objects=model_objects)\n",
    "    \n",
    "    df = pd.read_csv(os.path.join(PATH, folder + '/valid.csv'))\n",
    "    y_true = df['target'].to_list()\n",
    "    y_pred = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, pred in enumerate(preds):\n",
    "        total += 1\n",
    "        p = np.argmax(pred)\n",
    "        y_pred.append(p)\n",
    "        if p == y_true[i]:\n",
    "            correct += 1\n",
    "    print (folder, ': ', correct/total)\n",
    "    results_chemprop[folder] = correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685e6e82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7e7b3c0",
   "metadata": {},
   "source": [
    "# 4. Ablation Studies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b237f167",
   "metadata": {},
   "source": [
    "## Ablation: Replacement test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b202070f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacement test\n",
    "path = r'C:\\Users\\shibi\\PycharmProjects\\gptchem\\out\\new_data_gpt\\small_molecule\\20230701_152139__0.4_LUMO_1'\n",
    "model_id = 'ada:ft-birmingham-digital-chemistry-2023-07-03-13-39-02'\n",
    "attacker = Attacker(model_id)\n",
    "\n",
    "result = attacker.replacement_test(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72111d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacement test result collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be05c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5dda4073",
   "metadata": {},
   "source": [
    "## Ablation: Combine core data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06641bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORE_LISTS = [\n",
    "    ['core_16392',\n",
    "     'core_16404',\n",
    "     'core_16583',\n",
    "     'core_14647',\n",
    "     'core_14707'\n",
    "    ], \n",
    "    ['core_7123',\n",
    "     'core_7188',\n",
    "     'core_8000'\n",
    "    ],\n",
    "    ['core_11149',\n",
    "     'core_11127',\n",
    "     'core_11138'\n",
    "    ],\n",
    "    ['core_16392',\n",
    "     'core_16404',\n",
    "     'core_16583',\n",
    "     'core_14647',\n",
    "     'core_14707',\n",
    "     'core_7123',\n",
    "     'core_7188',\n",
    "     'core_8000'\n",
    "    ],\n",
    "    ['core_16392',\n",
    "     'core_16404',\n",
    "     'core_16583',\n",
    "     'core_14647',\n",
    "     'core_14707',\n",
    "     'core_7123',\n",
    "     'core_7188',\n",
    "     'core_8000',\n",
    "     'core_11149',\n",
    "     'core_11127',\n",
    "     'core_11138'\n",
    "    ]\n",
    "]\n",
    "\n",
    "NAME_POSTFIXS = [\n",
    "    '12345',\n",
    "    '678',\n",
    "    '91011',\n",
    "    '12345678',\n",
    "    'all'\n",
    "]\n",
    "\n",
    "BASE_DIR = r'C:\\Users\\darkn\\PycharmProjects\\ChemGPT\\out\\new_data_gpt\\ablation'\n",
    "\n",
    "TYPES = ['LUMO', 'HOMO']\n",
    "\n",
    "def generate_core_combination_data(CORE_LIST, NAME):\n",
    "    for TYPE in TYPES:\n",
    "        test_list = []\n",
    "        for folder in os.listdir(BASE_DIR):\n",
    "            for df_name in CORE_LIST:\n",
    "                if TYPE in folder and df_name in folder:\n",
    "                    df = pd.read_csv(os.path.join(BASE_DIR, folder + '/valid.csv'))\n",
    "                    test_list.append(df)\n",
    "                    path = os.path.join(BASE_DIR, folder)\n",
    "        test_df = pd.concat(test_list)\n",
    "\n",
    "        train_df = pd.read_csv(os.path.join(path, 'train.csv'))\n",
    "        duplicate = train_df.merge(test_df, on=['smiles', 'target'])\n",
    "        train_df = train_df.append(duplicate)\n",
    "        train_df = train_df.drop_duplicates(subset='smiles', keep=False)\n",
    "        train_df.columns = ['prompt', 'completion']\n",
    "        test_df.columns = ['prompt', 'completion']\n",
    "        train_df['completion'] = train_df['completion'].astype(str)\n",
    "        test_df['completion'] = test_df['completion'].astype(str)\n",
    "        train_df.to_csv('train_core_{}_{}.csv'.format(TYPE, NAME), index=False)\n",
    "        test_df.to_csv('test_core_{}_{}.csv'.format(TYPE, NAME), index=False)\n",
    "        write_file(train_df, test_df, '{}_{}'.format(TYPE, NAME))\n",
    "        \n",
    "for CORE_LIST, NAME in zip(CORE_LISTS, NAME_POSTFIXS):\n",
    "    generate_core_combination_data(CORE_LIST, NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
